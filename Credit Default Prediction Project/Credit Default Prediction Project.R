Skip to content
This repository
Search
Pull requests
Issues
Marketplace
Explore
 @jingchu0918
 Sign out
 Watch 0
  Star 0  Fork 0 jingchu0918/jingchu_showcase
 Code  Issues 0  Pull requests 0  Projects 0  Wiki  Settings Insights 
Branch: master Find file Copy pathjingchu_showcase/Credit Default Prediction Project.R
796722a  a day ago
@jingchu0918 jingchu0918 Add files via upload
1 contributor
RawBlameHistory     
561 lines (471 sloc)  19.9 KB
####================= Chinese University of Hong Kong =========================####
####=============== RMSC4002 (16-17 FALL) Group Project =======================####
#                                                                                 #
# Notice:                                                                         #
# 1. Please make sure you R is the most updated version, or some of the functions #
# would not be able to use.                                                       #
#                                                                                 #
# 2. It is suggested to set run.ann to false in order to directly load the RData  #
# containing the existing ANN models which were already built by us, and to       #
# prevent running all the ANN models again. Be sure the file 'ann-models.RData'   #
# is in the working directory if you decide to load the existing models from the  #
# RData file.                                                                     #
#                                                                                 #
# 3. The plots generated by the code will be automatically saved into the file    #
# "RMSC4002_plots" in the working directory. No plot will be shown in the viewer  #
# while running the code                                                          #
#                                                                                 #
####===========================================================================####

#############
#           # 
# Settings  #
#           #
#############

save.to.dir <- file.path(getwd(),'RMSC4002_plots')
dir.create(save.to.dir, showWarnings = F)
run.ann <- F #If set to False, then load the existing RData containing our 
             #ANN models. If set to True, then run all the ANN again, 
             #which will be time consuming

##########################################
#                                        #
# Install (if needed) and load packages  #
#                                        #
##########################################
if(!require('rpart')) install.packages('rpart')
if(!require('rpart.plot')) install.packages('rpart.plot')
if(!require('MLmetrics')) install.packages('MLmetrics')
if(!require('tibble')) install.packages('tibble')
if(!require('ggplot2')) install.packages('ggplot2')
if(!require('plyr')) install.packages('plyr')
if(!require('reshape')) install.packages('reshape')
if(!require('nnet')) install.packages('nnet')
if(!require('bitops')) install.packages('bitops')
if(!require('RCurl')) install.packages('RCurl')

library(MLmetrics)
library(rpart)
library(rpart.plot)
library(ggplot2)
library(plyr)
library(reshape)
library(nnet)
library(bitops)
library(RCurl)

##############
#            #
# Functions  #
#            #
##############

#Mahalanobis distance
mdist<-function(x) {
  t<-as.matrix(x) # transform x to a matrix
  m<-apply(t,2,mean) # compute column mean
  s<-var(t) # compute sample covariance matrix
  mahalanobis(t,m,s) # using built-in mahalanobis function
}

#Save plot to directory
save.plot <- function(plot, name="Rplot", dir=save.to.dir,type='png',
                      width = NULL, height = NULL) {
  dir.create(dir, showWarnings = F)
  if(name=="Rplot")
    warning("You are using the default filename, file overwriting is possible.
            Better name the file by yourself.")
    
  filedir = file.path(dir, paste(name, type, sep='.'))
  
  if (type == 'pdf') {
    if (is.null(width)) width = 7
    if (is.null(height)) height = 7
    pdf(filedir, width = width, height = height)
  }
   
  else {
    if (is.null(width)) width = 480
    if (is.null(height)) height = 480
    if (type=='png')
      png(filedir, width = width, height = height)
    else if (type == 'jpg' || type == 'jpeg')
      jpeg(filedir, width = width, height = height)
    else if (type == 'bmp')
      bmp(filedir, width = width, height = height)
    else
    warning("File type can only be 'png', 'pdf', 'jpg', 'jpeg', 'bmp'")
  }

  print(plot) #Note, in the function, you must explicitly print the plot
  dev.off()
}

#Data Standardization
std <- function(x,method='minmax') {
  xn = nrow(x)
  mat1 = rep(1,xn)
  if (method=='minmax') {
    xminmat = mat1%*%t(apply(x,2,min))
    xmaxmat = mat1%*%t(apply(x,2,max))
    xstand = (x-xminmat)/(xmaxmat-xminmat)
  }
  else if (method=='meansd') {
    xsdmat = mat1%*%t(sqrt(apply(x,2,var)))
    xmeanmat = mat1%*%t(apply(x,2,mean))
    xstand = (x-xmeanmat)/xsdmat
  }
  else
    return (x)
  xstand
}

#ANN
ann <- function(x,y,size,maxit,linout,try) {
  ann1<-nnet(x,y,size=size,maxit=maxit,linout=linout)
  v1<-ann1$value  # save the value for the first trial
  for (i in 2:try) {
    ann<-nnet(x,y,size=size,maxit=maxit,linout=linout)
    if (ann$value<v1) {                                       # check if the current value is better
      v1<-ann$value  # save the best value
      ann1<-ann      # save the results
    }
  }
  ann1 # return the results
} 

#Calculate the area ratio of the lift chart (see the report for detail)
area_ratio <- function(pr,y){
  ysort<-y[order(pr,decreasing=T)]
  perc<-cumsum(ysort)/sum(ysort)
  pop<-(1:length(ysort))/length(ysort)
  perfect<-cumsum(sort(ysort,decreasing = TRUE))/sum(ysort)
  return((Area_Under_Curve(pop,perc)-0.5)/(Area_Under_Curve(pop,perfect)-0.5))
}

#Get the data frame of three cumsums for the lift chart 
#Parameters:
#pr: the vector of predicted probabilities
#y: the vector of actual categories (must be either 0 or 1)
get_lift_vec <- function(pr, y){
  if(length(pr) != length(y))
    stop('The two parameters must have the same length.')

  n <- length(pr) 
  n1 <- sum(y==1)
  n0 <- n - n1
  ysort <- y[order(pr,decreasing=T)]
  baseline <- (1:n)/n
  perfect <- cumsum(c(rep(1,n1),rep(0,n0)))/sum(y)
  model <- cumsum(ysort)/sum(y)
  return(data.frame(perfect, model, baseline))
}

#################
#               #
# Read in data  #
#               #
#################
d <- read.csv("default.csv")
varnames.ini <- names(d)
nobs.ini <- nrow(d)
nY0.ini <- nrow(d[d$Y==0,]) #Y==1: default. Y==0: not default
nY1.ini <- nrow(d[d$Y==1,])

#Covariance
dnonf <- d[,-c(1,3,5,25)]
z <- cor(dnonf)     # obtain the correlation matrix
z.m <- melt(z)  # Cartesian product
cormatrix <- ggplot(z.m, aes(X1, X2, fill = value)) +
  geom_tile() + scale_fill_gradient(low = "yellow", high = "purple") +
  labs(x='Variables',y='Variables',title = 'Correlation heatmap')
  
save.plot(cormatrix,'Correlation heatmap', width=600, height=480)

#################
#               #
# Data cleaning #
#               #
#################

#######################################
# Step 1: Outlier elimination (mdist) #
#######################################

x <- subset(d, select = -c(ID,SEX,EDUCATION, MARRIAGE,Y)) # save d to x
md<-mdist(x) # compute mdist
mdx <- data.frame(index=1:length(md),md=md)
c<-qchisq(0.99,df=ncol(x)) # p=ncol(x), and type-I error = 0.01
mdist_before <- ggplot(mdx, aes(y = md, x = index)) + geom_point() + 
  geom_hline(yintercept = c, col='red')

dclean<-d[md<c,]
x2 <- x[md<c,]
#To show that the outlier elimination doesn't effect the data much
nY0 <- nrow(dclean[dclean$Y==0,])
nY1 <- nrow(dclean[dclean$Y==1,])
nobs <- nrow(dclean)
propY1.ini <- nY1.ini/nobs.ini
propY1 <- nY1/nobs

md2 <- mdist(x2)
mdx2 <- data.frame(index=1:length(md2),md=md2)
mdist_no_outlier <- ggplot(mdx2, aes(y = md, x = index)) + geom_point()

save.plot(mdist_before, 'mdist_before')
save.plot(mdist_no_outlier, 'mdist_no_outlier')

#####################################
#  Step 2: Varaible reduction (PCA) #
#####################################

d<-dclean
BA.cor <- cor(d[,c("BILL_AMT1","BILL_AMT2","BILL_AMT3",
                   "BILL_AMT4","BILL_AMT5","BILL_AMT6")])
####correlations are close to 1
####PCA BILL_AMT
BA<-d[,c("BILL_AMT1","BILL_AMT2","BILL_AMT3","BILL_AMT4",
         "BILL_AMT5","BILL_AMT6")]
pca<-princomp(BA,cor=T)  #perform pca using correlation matrix
#and save the result to the object pca
s<-pca$sdev #save the s.d. of all PC to s
pca.var <- round(s^2,4)             #display variance
t<-sum(s^2)       #compute total variance
pca.prop <- round(s^2/t,4)    #proportion of variance explained by each PC
pca.cum <- cumsum(s^2/t)    ##cumulative sum of proportion of variance  

compvar <- melt(s^2)
compvar$index <- rownames(compvar)
scree <- ggplot(compvar,aes(x=index, y=value, group=1)) + 
  geom_point() + 
  geom_line() + 
  labs(x='',y='variance explained', title='Screeplot')

save.plot(scree, 'screeplot', width=800, height=400)

####we use the 1st PC to represent all the 6 variables
pcload <- as.data.frame(pca$loadings[,1:3]) #PC1 parallel shift, PC2 tilt, PC3 curvature
pcload$varnames=rownames(pcload)

pc1plot <- ggplot(pcload, aes(x = varnames, y=Comp.1, group=1)) + 
  geom_point() + geom_line() + lims(y=c(-0.6,0.6)) +
  labs(x="Variable names",y="loadings",title="1st pc loadings")

pc2plot <- ggplot(pcload, aes(x = varnames, y=Comp.2, group=1)) + 
  geom_point() + geom_line() + lims(y=c(-0.6,0.6)) +
  labs(x="Variable names",y="loadings",title="2nd pc loadings")

pc3plot <- ggplot(pcload, aes(x = varnames, y=Comp.3, group=1)) + 
  geom_point() + geom_line() + lims(y=c(-0.6,0.6)) +
  labs(x="Variable names",y="loadings",title="3rd pc loadings")


save.plot(pc1plot,'pc1plot',width=800,height=200)
save.plot(pc2plot,'pc2plot',width=800,height=200)
save.plot(pc3plot,'pc3plot',width=800,height=200)


pc1<-pcload[,1]*(-1)   ###since all the loadings are negative
BA<-as.matrix(BA)
BA<-BA %*% pc1
clean2 <- d
clean2[,c("BILL_AMT1")]<-BA
clean2<- subset(clean2,select=-c(BILL_AMT2,BILL_AMT3,
                                 BILL_AMT4,BILL_AMT5,BILL_AMT6))
names(clean2)[grep("BILL_AMT1",names(clean2))]<-"BILL_AMT"

################################
# Step 3: Data standardization #
################################

x <- subset(clean2,select=-c(ID))
xfac <- subset(x,select=c(SEX,EDUCATION, MARRIAGE))
x1 <- subset(x,select=6:11)
x2 <- subset(x,select=c(1,5,12:18))
xstand <- cbind(xfac,std(x1,method='minmax'),std(x2,method='meansd'),Y=x[,'Y'])
varnames.clean <- names(xstand)


#################################
#                               #
# Training and testing datasets #
#                               #
#################################
d <- xstand
set.seed(12345)
id<-sample(1:nrow(d),size=20000) # generate id
d1<-d[id,] # training dataset
d2<-d[-id,] # testing dataset


#######################
#                     #
# Classification tree #
#                     #
#######################

#training dataset
ctree<-rpart(Y~.,data=d1,method="class")
prt.ctree <- print(ctree)

#plot ctree
png(file.path(save.to.dir, 'Ctree.png'), width=300,height=300)
prp(ctree,type=3,extra=1,under=T)
dev.off()

#train result
pr.train<-predict(ctree)
cl.train<-max.col(pr.train)
ctree.table.train<-table(cl.train,d1$Y)
ctree.error.rate.train<-1-sum(diag(ctree.table.train)/sum(ctree.table.train))

#testing dataset
#test result
pr.test<-predict(ctree,d2)
cl.test<-max.col(pr.test)
ctree.table.test<-table(cl.test,d2$Y)
ctree.error.rate.test<-1-sum(diag(ctree.table.test)/sum(ctree.table.test))

#Plot PAY_0 - Y
pay0y_count <- ggplot(xstand,aes(x=factor(PAY_0),fill=factor(Y)), color=Y) + 
  geom_bar() +  #stat = count
  labs(x="PAY_0 (standardized)",y="Count",
       title="Total number of y=0 and 1 in each value of PAY_0")

pay0y_prop <- ggplot(xstand,aes(x=factor(PAY_0),fill=factor(Y)), color=Y) + 
  geom_bar(position="fill") + #stat = count
  labs(x="PAY_0 (standardized)",y="Proportion",
       title="Proportion of y=0 and 1 in each value of PAY_0")

save.plot(pay0y_count, name='PAY0_Y_count', width = 800, height=480)
save.plot(pay0y_prop, name='PAY0_Y_prop', width = 800, height=480)



#######################
#                     #
# logistic regression #
#                     #
#######################
dd<-xstand
dd[,c(1,3)] <- lapply(dd[,c(1,3)],factor)
g<-(dd$PAY_0>0.4375)+1 
dd<-cbind(dd,g)
n<-nrow(dd) # no. of obs
set.seed(12345) # set random seed
id<-sample(1:n,size=20000) # generate id
dd1<-dd[id,] # training dataset
dd2<-dd[-id,] # testing dataset

lreg<-glm(Y~.-PAY_0+PAY_2*g+PAY_3*g+PAY_4*g+PAY_5*g+PAY_6*g+
            BILL_AMT*g+PAY_AMT1*g+PAY_AMT2*g+PAY_AMT3*g+
            PAY_AMT4*g+PAY_AMT5*g+PAY_AMT6*g,data=dd1, binomial)
step(lreg) # perform stepwise selection

lreg<-glm(formula = Y ~ SEX + EDUCATION + MARRIAGE +
            PAY_2 + PAY_3 + PAY_4 + PAY_6 + LIMIT_BAL + AGE + BILL_AMT +
            PAY_AMT1 + PAY_AMT2 + PAY_AMT3 + PAY_AMT4 + PAY_AMT5 + PAY_AMT6 +
            g + PAY_2:g + PAY_6:g + BILL_AMT:g + PAY_AMT2:g,
          family = binomial, data = dd1)
lregsum <- summary(lreg)

pr1<-(lreg$fit>0.5)+1 # pred
lreg.traintable <- table(pr1,d1$Y)
lreg.error.train <- (lreg.traintable[2]+lreg.traintable[3])/sum(lreg.traintable)

c<-predict(lreg,dd2) # return x'b
pi<-exp(c)/(1+exp(c)) # estimate prob. of success
pr2<-(pi>0.5)+1 # prediction
lreg.testtable <- table(pr2,dd2$Y)
lreg.error.test <- (lreg.testtable[2]+lreg.testtable[3])/sum(lreg.testtable)

lreg_lch_train <- get_lift_vec(lreg$fit, dd1$Y)
lreg_lch_test <- get_lift_vec(pi, dd2$Y)

#plot
liftchart1 <- data.frame(pop=lreg_lch_train$baseline,lreg_lch_train)
lcmt1 <- melt(liftchart1,id.vars='pop')
pl <- ggplot(lcmt1, aes(x = pop, y=value, color=variable)) + 
  geom_line(size=1.5) + scale_color_manual(values=c("#3399FF","#003399", "#66CCFF"))
lreg.lift.train <- pl +  labs(x="",y="Cumulative Percentage",title="Lift chart for training data (logistic regression)") +
  theme(legend.title=element_blank())

liftchart2 <- data.frame(pop=lreg_lch_test$baseline,lreg_lch_test)
lcmt2 <- melt(liftchart2,id.vars='pop')
pl <- ggplot(lcmt2, aes(x = pop, y=value, color=variable)) + 
  geom_line(size=1.5) + scale_color_manual(values=c("#3399FF","#003399", "#66CCFF"))
lreg.lift.test <- pl +  labs(x="",y="Cumulative Percentage",title="Lift chart for testing data (logistic regression)") +
  theme(legend.title=element_blank())

save.plot(lreg.lift.train, name='lreg_liftchart_train', width = 800, height=480)
save.plot(lreg.lift.test, name='lreg_liftchart_test', width = 800, height=480)



###################################
#                                 #
# Artificial Neural Network (ANN) #
#                                 #
###################################
if(!run.ann){
  load("ann-models.RData") #ann6, ann7, ..., ann28, ann32, ann36
} else { #Warning: Time consuming!
  x<-d1[,1:18]
  ann6<-ann(x,d1$Y,size=6,maxit=500,linout=F,try=5)
  ann8<-ann(x,d1$Y,size=8,maxit=500,linout=F,try=5) 
  ann10<-ann(x,d1$Y,size=10,maxit=500,linout=F,try=5)
  ann12<-ann(x,d1$Y,size=12,maxit=500,linout=F,try=5) 
  ann14<-ann(x,d1$Y,size=14,maxit=500,linout=F,try=5)
  ann16<-ann(x,d1$Y,size=16,maxit=500,linout=F,try=5) 
  ann17<-ann(x,d1$Y,size=17,maxit=500,linout=F,try=5)
  ann18<-ann(x,d1$Y,size=18,maxit=500,linout=F,try=5)
  ann19<-ann(x,d1$Y,size=19,maxit=500,linout=F,try=5)
  ann20<-ann(x,d1$Y,size=20,maxit=500,linout=F,try=5) 
  ann21<-ann(x,d1$Y,size=21,maxit=500,linout=F,try=5) 
  ann22<-ann(x,d1$Y,size=22,maxit=500,linout=F,try=5)
  ann23<-ann(x,d1$Y,size=22,maxit=500,linout=F,try=5)
  ann24<-ann(x,d1$Y,size=24,maxit=500,linout=F,try=5)
  ann25<-ann(x,d1$Y,size=25,maxit=500,linout=F,try=5)
  ann26<-ann(x,d1$Y,size=26,maxit=500,linout=F,try=5)
  ann28<-ann(x,d1$Y,size=28,maxit=500,linout=F,try=5)
  ann32<-ann(x,d1$Y,size=32,maxit=500,linout=F,try=5)
  ann36<-ann(x,d1$Y,size=36,maxit=500,linout=F,try=5)
}

#Get the data.frame containing the area ratio of train and test for each ann model
area_ratios <- data.frame(size=NULL, train=NULL, test=NULL)
for(i in c(6:28,32,36)) {
  objname <- paste('ann', i, sep='')
  annmod <- eval(parse(text = objname))
  predtrain <- annmod$fit
  trainratio <- area_ratio(predtrain, d1$Y)
  
  predtest <- predict(annmod,d2)
  testratio <- area_ratio(predtest, d2$Y)
  area_ratios <- rbind(area_ratios, data.frame(size=i, train=trainratio, test=testratio))
}

#Plot - area ratio
ratiosmt <- melt(area_ratios, id.vars='size')
pl <- ggplot(ratiosmt, aes(x = size, y=value, color=variable)) + 
  geom_line(size=1) + geom_point()
area.ratio.trend <- pl +  labs(x="hiden layer size",y="area ratio",title="Area Ratio vs Size") +
  theme(legend.title=element_blank())

save.plot(area.ratio.trend, 'area_ratio_trend', width=800, height = 480)

#Classification table
ann.pr.train<-ann12$fit        #fitted value of the training data set
ann.table.train <- table(round(ann.pr.train),d1$Y)  #classification table for d1
ann.error.train <- (ann.table.train[1,2]+ann.table.train[2,1])/(sum(ann.table.train)) #compute training error rate

ann.pr.test<-predict(ann12,d2)        #apply the ann12 to testing data d2
ann.table.test <- table(round(ann.pr.test),d2$Y)  #classification table for d2
ann.error.test <- (ann.table.test[1,2]+ann.table.test[2,1])/(sum(ann.table.test)) #compute testing error rate

#Lift chart
ann_lch_train <- get_lift_vec(ann.pr.train, d1$Y)
ann_lch_test <- get_lift_vec(ann.pr.test, d2$Y)

#plot
ann.liftchart1 <- data.frame(pop=ann_lch_train$baseline,ann_lch_train)
ann.lcmt1 <- melt(ann.liftchart1,id.vars='pop')
pl <- ggplot(ann.lcmt1, aes(x = pop, y=value, color=variable)) + 
  geom_line(size=1.5) + scale_color_manual(values=c("#3399FF","#003399", "#66CCFF"))
ann.lift.train <- pl +  labs(x="",y="Cumulative Percentage",title="Lift chart for training data (ANN)") +
  theme(legend.title=element_blank())

ann.liftchart2 <- data.frame(pop=ann_lch_test$baseline,ann_lch_test)
ann.lcmt2 <- melt(ann.liftchart2,id.vars='pop')
pl <- ggplot(ann.lcmt2, aes(x = pop, y=value, color=variable)) + 
  geom_line(size=1.5) + scale_color_manual(values=c("#3399FF","#003399", "#66CCFF"))
ann.lift.test <- pl +  labs(x="",y="Cumulative Percentage",title="Lift chart for testing data (ANN)") +
  theme(legend.title=element_blank())

save.plot(ann.lift.train, name='ann_liftchart_train', width = 800, height=480)
save.plot(ann.lift.test, name='ann_liftchart_test', width = 800, height=480)


#ANN model visualization
root.url<-'https://gist.githubusercontent.com/fawda123'
raw.fun<-paste(
  root.url,
  '5086859/raw/cc1544804d5027d82b70e74b83b3941cd2184354/nnet_plot_fun.r',
  sep='/'
)
script<-getURL(raw.fun, ssl.verifypeer = FALSE)
eval(parse(text = script))
rm('script','raw.fun')
y<-d1$Y

png(file.path(save.to.dir, 'Network.png'),width=1000,height=650)
plot.nnet(ann12,cex.val=0.85,pos.col='#996666',neg.col='#003399',circle.cex=2,cex=1.4,circle.col='#99CCCC')
dev.off()



#######################
#                     #
# Outputs and results #
#                     #
#######################
cat('Names of variables in the raw data set:\n', varnames.ini,
    '\nTotal number of observations in the raw data set:', nobs.ini,
    '\nNumber of Y==1 in the raw data set:', nY1.ini, ', Proportion:', propY1.ini,
    '\nTotal number of observations in the truncated set:', nobs,
    '\nNumber of Y==1 in the truncated data set:', nY1, ', Proportion:', round(propY1,4),
    '\nThe truncation based on Mahalanobis distance does not affect much.')

cat('PCA results:\n',
    'Variance of each PC:', pca.var,
    '\nProportion of variance explained by each PC:', pca.prop,
    '\nCumulative sum of proportion of variance:', pca.cum)

cat('Names of variables in the clean data set:\n', varnames.clean)

#ctree
cat('Ctree:\n')
prt.ctree
cat('Ctree: Classification table for training data set:')
ctree.table.train
cat('Ctree: Error rate for training data set:', ctree.error.rate.train)
cat('Ctree: Classification table for testing data set:')
ctree.table.test
cat('Ctree: Error rate for testing data set:', ctree.error.rate.test)

#Lreg
cat('Logistic regression: summary of the final model')
lregsum

cat('Lreg: Classification table for training data set:')
lreg.traintable
cat('Lreg: Classification table for testing data set:')
lreg.testtable

#ANN
cat('ANN: Area ratios:')
area_ratios
cat('ANN: \nSelected network: 1 layer, 12 hidden nodes.')
cat('ANN: Classification table for training data set:')
ann.table.train
cat('ANN: Error rate for training data set:', ann.error.train,
    '\nANN: Area ratio for training data set:', area_ratios[12,'train'])
cat('ANN: Classification table for testing data set:')
ann.table.test
cat('ANN: Error rate for testing data set:,', ann.error.test,
    '\nANN: Area ratio for testing data set:', area_ratios[12,'test'])



© 2017 GitHub, Inc.
Terms
Privacy
Security
Status
Help
Contact GitHub
API
Training
Shop
Blog
About
